# -*- coding: utf-8 -*-
"""seir-hcd-model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dBYFOx5BVeFjPb3fugPeWrCV0CjWphc4
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import json, os
import matplotlib.pyplot as plt
from sklearn.linear_model import BayesianRidge
from sklearn.externals import joblib

from datetime import date
from sklearn.model_selection import GridSearchCV
from sklearn import preprocessing
from sklearn.neural_network import MLPRegressor
from sklearn.base import clone

from helpers import metrics_report
from simulations import features, periods
country_name = "Luxembourg"


"""# Fitting the model to data
There are certain variables that we can play with to fit the model to real data:
* Average incubation period, `t_inc`
* Average infection period, `t_inf`
* Average hospitalization period, `t_hosp`
* Average critital period, `t_crit`
* The fraction of mild/asymptomatic cases, `m_a`
* The fraction of severe cases that turn critical, `c_a`
* The fraction of critical cases that result in a fatality, `f_a`
* Reproduction number, `R_0` or `R_t`
"""

current_dataset_date = date(2020,5,10).strftime("%Y_%m_%d")
dataset= pd.read_csv("./datasets/v2_1_google.csv", parse_dates=['Date'])
dataset = dataset.drop(["Unnamed: 0"],axis=1)

"""## ML to predict Reproduction Rate"""

current_dataset_date = "v2_1"
all_countries= pd.read_csv("datasets/v2_1_seirhcd.csv", parse_dates=['Date'])

dataset = pd.get_dummies(dataset,prefix="day_of_week", columns=["day_of_week"])
dataset = pd.get_dummies(dataset,prefix="region", columns=["region"])
merged = pd.merge(all_countries.groupby(["CountryName","Date"]).agg("first"), dataset.groupby(["CountryName","Date"]).agg("first"),  on=["CountryName","Date"], how="inner")
merged = merged.reset_index().dropna()
print(merged.describe())
#print(merged.isnull())
#print(merged.head(1),merged.tail(1))

merged_columns = list(merged.columns)

columns = ["{}{}".format(f,p) for p in periods for f in features]

columns = columns + ["density","population","population_p65","population_p14","gdp","area"]
columns = columns + ["day_of_week_{}".format(i) for i in range(7)]
columns = columns + ["region_{}".format(i) for i in range(10)]
merged = merged.rename(columns={'region_10':'region_9'})
country_names = ["Luxembourg","France","Germany","Spain","United kingdom","Greece","Italy","Switzerland","Latvia","Belgium","Netherlands"]
country = merged[merged["CountryName"].isin(country_names)]
non_country = merged[~merged["CountryName"].isin(country_names)]
non_country = merged

all_countries = merged["CountryName"].unique()

X_train, y_train = non_country[columns], non_country["R"]
X_test, y_test = country[columns], country["R"]

scaler = preprocessing.StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
print("train in {} elements, test on {} elements".format(X_train.shape,X_test.shape))
print("min-max",X_train_scaled.min(axis=1),X_train_scaled.max(axis=1))

from copy import deepcopy
best_perf = -1
best_model = None
search = False

nb_iters = 1 if search else 5

for i in range(nb_iters):
    print("Iter search {}".format(i))
    parameter_space = {
        'hidden_layer_sizes': [(1000,50),(50, 100, 50), (50, 100, 100), (50, 500, 50)],
        'alpha': [0.0001, 0.05]
    }

    mlp = MLPRegressor((1000,50),max_iter=1500, verbose=True, solver="adam")
    if search:
        mlp_clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3, verbose=True)
    else:
        mlp_clf = mlp
    mlp_clf.fit(X_train_scaled,y_train.values)

    reports, reports_train = metrics_report(X_test_scaled, y_test, mlp_clf),metrics_report(X_train_scaled, y_train, mlp_clf)
    print("ref",best_perf,reports)

    if reports["r2_score"]>best_perf:
        best_model = deepcopy(mlp_clf.best_estimator_ if mlp!=mlp_clf else mlp)
        best_perf =  reports["r2_score"]

    if reports["r2_score"]>0.95:
        break

y_mlp = best_model.predict(X_test_scaled)
reg = BayesianRidge(compute_score=True, tol=1e-5)

parameters = {'alpha_init':(0.2, 0.5, 1, 1.5), 'lambda_init':[1e-3, 1e-4, 1e-5,1e-6]}
srch = GridSearchCV(reg, parameters)

srch.fit(X_train, y_train)

params = srch.get_params()
reg.set_params(alpha_init=params["estimator__alpha_init"], lambda_init=params["estimator__lambda_init"])
reg.fit(X_train, y_train)
ymean, ystd = reg.predict(X_test, return_std=True)



folder = "./models/seirhcd/{}".format(current_dataset_date)
os.makedirs(folder, exist_ok=True)

joblib.dump(best_model, '{}/mlp.save'.format(folder))
joblib.dump(scaler, "{}/scaler.save".format(folder))

with open('{}/metrics.json'.format(folder), 'w') as fp:
    json.dump({"perf":reports,"std_test":list(ystd.values), "columns":columns, "countries":list(all_countries), "hidden_layer_sizes":best_model.hidden_layer_sizes}, fp)

merged.to_csv('{}/features.csv'.format(folder))


yvar =np.sqrt(ystd)
#yvar = ystd

plt.figure(figsize=(30,10))

plt.plot(np.arange(len(X_test)), y_mlp, color="red", label="predict mlp")
plt.plot(np.arange(len(X_test)), y_test, color="blue", label="ground truth")

plt.fill_between(np.arange(len(X_test)), y_mlp-yvar/2, y_mlp+yvar/2,
                color="pink", alpha=0.5, label="Confidence interval")
plt.tight_layout()
plt.legend()

plt.xticks(np.arange(len(X_test)),country["Date"].dt.strftime('%d/%m/%Y'), rotation=90)
plt.show()